{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "# from rake_nltk import Rake\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train_feature/train2yu_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('train_feature/test2yu_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper(x):\n",
    "    jj=x['journal']\n",
    "    text=x['description_text']\n",
    "    if jj in text:\n",
    "        return 1\n",
    "    return 0\n",
    "def get_year(x):\n",
    "    try:\n",
    "        \n",
    "        jj=str(int(x['year']))\n",
    "        text=x['description_text']\n",
    "        if jj in text:\n",
    "            return 1\n",
    "        return 0\n",
    "    except:\n",
    "        return x\n",
    "def get_year_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p=pd.concat([train,test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p['is_have_paper']=data_p.apply(get_paper,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_p['is_have_year']=data_p.apply(get_year,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p['year']=data_p['year'].apply(get_year_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_sim(train_query_tf,train_title_tf):\n",
    "    # 余弦\n",
    "    v_num = np.array(train_query_tf.multiply(train_title_tf).sum(axis=1))[:, 0]\n",
    "    v_den = np.array(np.sqrt(train_query_tf.multiply(train_query_tf).sum(axis=1)))[:, 0] * np.array(\n",
    "            np.sqrt(train_title_tf.multiply(train_title_tf).sum(axis=1)))[:, 0]\n",
    "    v_num[np.where(v_den == 0)] = 1\n",
    "    v_den[np.where(v_den == 0)] = 1\n",
    "    v_score1 = 1 - v_num / v_den\n",
    "\n",
    "    # 欧式\n",
    "    v_score = train_query_tf - train_title_tf\n",
    "    v_score2 = np.sqrt(np.array(v_score.multiply(v_score).sum(axis=1))[:, 0])\n",
    "\n",
    "    # 曼哈顿\n",
    "    v_score = np.abs(train_query_tf - train_title_tf)\n",
    "    v_score3 = v_score.sum(axis=1)\n",
    "\n",
    "    return  v_score1,v_score2,v_score3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = list(data_p['key_text_pre'].fillna(''))\n",
    "article.extend(list(data_p['title_pro'].fillna('')))\n",
    "article.extend(list(data_p['abstract_pre'].fillna('')))\n",
    "article.extend(list(data_p['description_text'].fillna('')))\n",
    "article.extend(list(data_p['title'].fillna('')))\n",
    "article.extend(list(data_p['abstract'].fillna('')))\n",
    "article=list(set(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TfidfVectorizer(ngram_range=(1,1), min_df=2, max_df=0.5, sublinear_tf=True)\n",
    "tft.fit(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query_tf = tft.transform(data_p['key_text_pre'].fillna('').values)\n",
    "train_query_tf2 = tft.transform(data_p['description_text'].fillna('').values)\n",
    "train_title_tf = tft.transform(data_p['title_pro'].fillna('').values)\n",
    "train_title_tf2 = tft.transform(data_p['abstract_pre'].fillna('').values)\n",
    "train_title_tf3 = tft.transform(data_p['title'].fillna('').values)\n",
    "train_title_tf4 = tft.transform(data_p['abstract'].fillna('').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "data_p['tfidf_cos'],data_p['tfidf_os'] ,data_p['tfidf_mhd']=get_tf_sim(train_query_tf,train_title_tf)\n",
    "print(0)\n",
    "data_p['tfidf_cos2'],data_p['tfidf_os2'] ,data_p['tfidf_mhd2']=get_tf_sim(train_query_tf,train_title_tf2)\n",
    "print(1)\n",
    "data_p['tfidf_cos_2'],data_p['tfidf_os_2'] ,data_p['tfidf_mhd_2']=get_tf_sim(train_query_tf2,train_title_tf)\n",
    "print(2)\n",
    "data_p['tfidf_cos2_2'],data_p['tfidf_os2_2'] ,data_p['tfidf_mhd2_2']=get_tf_sim(train_query_tf2,train_title_tf2)\n",
    "print(3)\n",
    "data_p['tfidf_cos2_3'],data_p['tfidf_os2_3'] ,data_p['tfidf_mhd2_3']=get_tf_sim(train_query_tf2,train_title_tf3)\n",
    "print(4)\n",
    "data_p['tfidf_cos2_4'],data_p['tfidf_os2_4'] ,data_p['tfidf_mhd2_4']=get_tf_sim(train_query_tf2,train_title_tf4)\n",
    "print(5)\n",
    "del  train_title_tf,train_query_tf,train_query_tf2,train_title_tf2,train_title_tf3,train_title_tf4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import  numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    '''\n",
    "        去除多余空白、分词、词性标注\n",
    "    '''\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    token_words = word_tokenize(sentence)  # 输入的是列表\n",
    "    token_words = pos_tag(token_words)\n",
    "    return token_words\n",
    "\n",
    "\n",
    "def stem(token_words):\n",
    "    '''\n",
    "        词形归一化\n",
    "    '''\n",
    "    wordnet_lematizer = WordNetLemmatizer()  # 单词转换原型\n",
    "    words_lematizer = []\n",
    "    for word, tag in token_words:\n",
    "        if tag.startswith('NN'):\n",
    "            word_lematizer = wordnet_lematizer.lemmatize(word, pos='n')  # n代表名词\n",
    "        elif tag.startswith('VB'):\n",
    "            word_lematizer = wordnet_lematizer.lemmatize(word, pos='v')  # v代表动词\n",
    "        elif tag.startswith('JJ'):\n",
    "            word_lematizer = wordnet_lematizer.lemmatize(word, pos='a')  # a代表形容词\n",
    "        elif tag.startswith('R'):\n",
    "            word_lematizer = wordnet_lematizer.lemmatize(word, pos='r')  # r代表代词\n",
    "        else:\n",
    "            word_lematizer = wordnet_lematizer.lemmatize(word)\n",
    "        words_lematizer.append(word_lematizer)\n",
    "    return words_lematizer\n",
    "\n",
    "\n",
    "sr = stopwords.words('english')\n",
    "\n",
    "\n",
    "def delete_stopwords(token_words):\n",
    "    '''\n",
    "        去停用词\n",
    "    '''\n",
    "    cleaned_words = [word for word in token_words if word not in sr]\n",
    "    return cleaned_words\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    '''\n",
    "        判断字符串是否为数字\n",
    "    '''\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "characters = [' ', ',', '.', 'DBSCAN', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%', '-', '...',\n",
    "              '^', '{', '}']\n",
    "\n",
    "\n",
    "def delete_characters(token_words):\n",
    "    '''\n",
    "        去除特殊字符、数字\n",
    "    '''\n",
    "    words_list = [word for word in token_words if word not in characters and not is_number(word)]\n",
    "    return words_list\n",
    "\n",
    "\n",
    "def to_lower(token_words):\n",
    "    '''\n",
    "        统一为小写\n",
    "    '''\n",
    "    words_lists = [x.lower() for x in token_words]\n",
    "    return words_lists\n",
    "\n",
    "\n",
    "def pre_process(text):\n",
    "    '''\n",
    "        文本预处理\n",
    "    '''\n",
    "    token_words = tokenize(text)\n",
    "    token_words = stem(token_words)\n",
    "    token_words = delete_stopwords(token_words)\n",
    "    token_words = delete_characters(token_words)\n",
    "    token_words = to_lower(token_words)\n",
    "    return token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "article=list(data_p['title'].fillna(''))\n",
    "article.extend(list(data_p['abstract'].fillna('')))\n",
    "content=list(set(article))\n",
    "train_content = list(map(lambda x: pre_process(x), tqdm(content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "#tfidf match share\n",
    "print('get tfidf match share:')\n",
    "\n",
    "def get_weight(count, eps=100, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "\n",
    "def load_weight(data):\n",
    "    words = [x for y in data for x in y.split()]\n",
    "    counts = collections.Counter(words)\n",
    "    weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "    del counts\n",
    "    del words\n",
    "    del data\n",
    "    gc.collect()\n",
    "    return weights\n",
    "\n",
    "\n",
    "def tfidf_match_share(queries, titles, weights):\n",
    "    ret = []\n",
    "    for i in tqdm(range(len(queries))):\n",
    "        q, t = str(queries[i]).split(), str(titles[i]).split()\n",
    "        q1words = {}\n",
    "        q2words = {}\n",
    "        for word in q:\n",
    "            q1words[word] = 1\n",
    "        for word in t:\n",
    "            q2words[word] = 1\n",
    "        if len(q1words) == 0 or len(q2words) == 0:\n",
    "            # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "            R = 0\n",
    "        else:\n",
    "            shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + \\\n",
    "                             [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "            total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "\n",
    "            R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "        ret.append(R)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [x for y in train_content for x in y]\n",
    "counts = collections.Counter(words)\n",
    "del train_content,content\n",
    "gc.collect()\n",
    "\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "del counts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('weights3.txt','r')\n",
    "# a = f.read()\n",
    "# weights = eval(a)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "data_p['tfidf_match_share'] = tfidf_match_share(data_p['key_text_pre'].values, data_p['title_pro'].values, weights)\n",
    "data_p['tfidf_match_share_pa'] = tfidf_match_share(data_p['key_text_pre'].values, data_p['abstract_pre'].values, weights)\n",
    "\n",
    "data_p['tfidf_match_share_2'] = tfidf_match_share(data_p['description_text'].values, data_p['title_pro'].values, weights)\n",
    "data_p['tfidf_match_share_pa_2'] = tfidf_match_share(data_p['description_text'].values, data_p['abstract_pre'].values, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p['tfidf_match_share_3'] = tfidf_match_share(data_p['description_text'].values, data_p['title'].values, weights)\n",
    "data_p['tfidf_match_share_pa_3'] = tfidf_match_share(data_p['description_text'].values, data_p['abstract'].values, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "data_p['journal'] = le.fit_transform(data_p['journal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p['paper_id_encode']=le.fit_transform(data_p['paper_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['key_text_pre','abstract','abstract_pre','title','title_pro','description_text']\n",
    "need_columns=[]\n",
    "for col in columns:\n",
    "    data_p[col+'+_len']=data_p[col].map(lambda x:len(str(x).split(' ')))\n",
    "    need_columns.append(col+'+_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz=data_p.groupby('paper_id')['description_id'].agg('count').reset_index()\n",
    "zz.columns=['paper_id','paper_id_count']\n",
    "data_p=pd.merge(data_p,zz,on='paper_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suanshu=['tfidf_cos','tfidf_os','tfidf_mhd','tfidf_cos2','tfidf_os2','tfidf_mhd2','tfidf_cos_2','tfidf_cos2_3','tfidf_os2_3','tfidf_mhd2_3','tfidf_cos2_4','tfidf_os2_4','tfidf_mhd2_4',\n",
    "            'tfidf_os_2','tfidf_mhd_2','tfidf_cos2_2','tfidf_os2_2','tfidf_mhd2_2','score','tfidf_match_share','tfidf_match_share_pa','tfidf_match_share_2','tfidf_match_share_pa_2',\n",
    "         'tfidf_match_share_3','tfidf_match_share_pa_3']\n",
    "new_columns=[]\n",
    "\n",
    "for i in range(len(suanshu)-1):\n",
    "    coli=suanshu[i]\n",
    "    for j in range(i+1,len(suanshu)):\n",
    "        colj=suanshu[j]\n",
    "        data_p[coli+'*'+colj]=data_p[coli]*data_p[colj]\n",
    "        new_columns.append(coli+'*'+colj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytrain=data_p[data_p.label.notnull()].reset_index(drop=True)\n",
    "mytest=data_p[data_p.label.isnull()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytrain=mytrain[mytrain.description_id!='6.45E+04'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytrain=mytrain.sort_values(['description_id', 'score'], ascending=[True, False]).reset_index(drop=True)\n",
    "mytest=mytest.sort_values(['description_id', 'score'], ascending=[True, False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rank=[]\n",
    "for i in range(len(mytest)//100):\n",
    "    test_rank.extend([i+1 for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rank=[]\n",
    "for i in range(len(mytrain)//100):\n",
    "    train_rank.extend([i+1 for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytrain['rank']=train_rank\n",
    "mytest['rank']=test_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p.columns,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_columns=['is_have_paper','tfidf_cos','tfidf_os','tfidf_mhd',\n",
    "            'tfidf_cos2','tfidf_os2','tfidf_mhd2','tfidf_cos_2','tfidf_cos2_3',\n",
    "            'tfidf_os2_3','tfidf_mhd2_3','tfidf_cos2_4','tfidf_os2_4','tfidf_mhd2_4',\n",
    "            'tfidf_os_2','tfidf_mhd_2','tfidf_cos2_2','tfidf_os2_2','tfidf_mhd2_2',\n",
    "            'score','rank','tfidf_match_share','tfidf_match_share_pa','tfidf_match_share_2',\n",
    "            'tfidf_match_share_pa_2','tfidf_match_share_3','tfidf_match_share_pa_3',\n",
    "            'journal','year','paper_id_encode','paper_id_count']+['key_text_pre+_len',\n",
    "                                                 'abstract+_len',\n",
    "                                                 'abstract_pre+_len',\n",
    "                                                 'title+_len',\n",
    "                                                 'title_pro+_len',\n",
    "                                                 'description_text+_len']+new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = mytrain[my_columns].values\n",
    "train_y = mytrain['label'].values\n",
    "test_x = mytest[my_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMClassifier(num_leaves=32, \n",
    "                             learning_rate=0.1,\n",
    "                             boosting='gbdt', \n",
    "                             max_depth=-1,\n",
    "                             bagging_fraction=0.8,\n",
    "                             n_estimators=40000,\n",
    "                             bagging_freq=1,\n",
    "                             feature_fraction=0.8, \n",
    "                             reg_alpha=0, \n",
    "                             reg_lambda=1,\n",
    "                             lambda_l1=1,\n",
    "                             lambda_l2=0.1,\n",
    "                             metric='auc',\n",
    "                             min_child_samples=5,\n",
    "                             objective='binary')        \n",
    "\n",
    "\n",
    "res = pd.DataFrame()\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=False, random_state=0)\n",
    "baseloss = []\n",
    "loss = 0\n",
    "for i, (train_index, test_index) in enumerate(skf.split(train_x,train_y)):\n",
    "    print(\"Fold\", i)\n",
    "    lgb_model = lgb_clf.fit(train_x[train_index], train_y[train_index],\n",
    "                          eval_names =['train','valid'],\n",
    "                          eval_metric='auc',\n",
    "                          eval_set=[(train_x[train_index], train_y[train_index]),\n",
    "                                    (train_x[test_index], train_y[test_index])],early_stopping_rounds=50)\n",
    "    baseloss.append(lgb_model.best_score_['valid']['auc'])\n",
    "    loss += lgb_model.best_score_['valid']['auc']\n",
    "    test_pred= lgb_model.predict_proba(test_x, num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "    print('test mean:', test_pred.mean())\n",
    "    res['prob_%s' % str(i)] = test_pred\n",
    "print('logloss:', baseloss, loss/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['prob'] = 0\n",
    "for i in range(5):\n",
    "    res['prob'] += res['prob_%s' % str(i)]\n",
    "res['prob'] = res['prob'] / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['description_id'] = mytest['description_id']\n",
    "res['paper_id'] = mytest['paper_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res[['description_id', 'paper_id', 'prob']]\n",
    "\n",
    "hh = res.sort_values(['description_id', 'prob'], ascending=[True, False])\n",
    "hh = hh.reset_index(drop=True)\n",
    "\n",
    "result = []\n",
    "for i in range(0, len(hh), 100):\n",
    "    line = hh.iloc[i:i + 3]\n",
    "    result.append(list(line['paper_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame(result)\n",
    "result.columns=['paper'+str(i) for i in range(1,11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "press2 = pd.read_csv('./data/test_pre2.csv')\n",
    "press2=press2.sort_values(by='description_id',ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['description_id']=press2['description_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[['description_id','paper1','paper2','paper3']].to_csv('result/lgb_result2_4.csv',index=None,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
